{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models\n",
    "\n",
    "This notebook picks up after the `simple_models` notebook. After trying a range of classification algorithms, we'll try out some of the neural network models in [1]. These include fully-connected models of varying layer sizes, and finally convolutional models including the famous LeNet-5. \n",
    "\n",
    "Along the way, we'll be using Keras which is a library sitting on top of Theano or Tensorflow. This allows easy construction, training and evaluation of neural nets. Before we get started, here's a recap of the `simple_models` notebook models.\n",
    "\n",
    "`[1]` - [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf),  LeCun et al, Nov 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in dependencies, may not use all of these\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pickle files\n",
    "\n",
    "The original data files are processed using the `convert_data.py` script, and written out to pickle files. We can load these in as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle files ...\n"
     ]
    }
   ],
   "source": [
    "# Set up the file directory and names\n",
    "DIR = '../input/'\n",
    "X_TRAIN = DIR + 'train-images-idx3-ubyte.pkl'\n",
    "Y_TRAIN = DIR + 'train-labels-idx1-ubyte.pkl'\n",
    "X_TEST = DIR + 't10k-images-idx3-ubyte.pkl'\n",
    "Y_TEST = DIR + 't10k-labels-idx1-ubyte.pkl'\n",
    "\n",
    "def load_data():\n",
    "    '''Loads pickled ubyte files with MNIST data\n",
    "    INPUT: X_train_file, y_train_file - strings with training filenames\n",
    "           X_test_file, y_test_File - strings with test filenames\n",
    "    RETURNS: Tuple with (X_train, y_train, X_test, y_test)\n",
    "    '''\n",
    "    print('Loading pickle files ...')\n",
    "    try:\n",
    "        X_train = pickle.load( open( X_TRAIN, \"rb\" ) )\n",
    "        y_train = pickle.load( open( Y_TRAIN, \"rb\" ) )\n",
    "        X_test = pickle.load( open( X_TEST, \"rb\" ) )\n",
    "        y_test = pickle.load( open( Y_TEST, \"rb\" ) )\n",
    "    except:\n",
    "        print('Error loading pickle file')\n",
    "        return None\n",
    "        \n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "X_train, y_train, X_test,  y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Before evaluating some models on the images, let's create some helper functions we can re-use later on. These deal with converting images to and from 1d and 2d versions, plotting images, resizing them, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train images shape (60000, 784), labels shape (60000, 1)\n",
      "Loaded test images shape (10000, 784), labels shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "def flatten_images(X):\n",
    "    ''' Converts images to 1-d vectors\n",
    "    INPUT: X - Input array of shape [n, w, h]\n",
    "    RETURNS: Numpy array of shape [n, w*h]\n",
    "    '''\n",
    "    n, w, h = X.shape\n",
    "    X_flat = X.reshape((n, w * h))\n",
    "    return X_flat\n",
    "\n",
    "def square_images(X, w, h):\n",
    "    '''Converts single-vector images into square images \n",
    "    INPUT: X - numpy array of images in single-vector form\n",
    "           w - width of images to convert to\n",
    "           h - height of images to convert to\n",
    "    RETURNS: Numpy array of shape [n, w, h]\n",
    "    '''\n",
    "    assert X.shape[1] == w * h, \"Error - Can't square array of shape {} to {}\".format(X.shape, (w, h))\n",
    "    n = X.shape[0]\n",
    "    X_square = X.reshape((n, w, h))\n",
    "    return X_square\n",
    "\n",
    "\n",
    "N_TRAIN, W, H = X_train.shape\n",
    "N_TEST, w_test, h_test = X_test.shape\n",
    "\n",
    "# Flatten the images\n",
    "X_train = flatten_images(X_train)\n",
    "X_test = flatten_images(X_test)\n",
    "\n",
    "# Do some checks on the data\n",
    "assert N_TRAIN == 60000, 'Error - expected 60000 training images, got {}'.format(N_TRAIN)\n",
    "assert N_TEST == 10000, 'Error - expected 60000 training images, got {}'.format(N_TEST)\n",
    "assert W == w_test, 'Error - width mismatch. Train {}, Test {}'.format(w, w_test)\n",
    "assert H == h_test, 'Error - height mismatch. Train {}, Test {}'.format(h, h_test)\n",
    "\n",
    "assert np.array_equal(X_train, flatten_images(square_images(X_train, W, H)))\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "print('Loaded train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Loaded test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "This section sets up global constants used in all models (to ensure a fair comparison). It also prepares the data by converting y values to one-hot, and normalizing X inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting y variables to one-hot encoding..\n",
      "Z-normalizing X data..\n",
      "Train images shape (60000, 784), labels shape (60000, 10)\n",
      "Test images shape (10000, 784), labels shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "\n",
    "# Keras Common configuration\n",
    "SEED = 1234 # Fix the seed for repeatability\n",
    "N_JOBS=-2 # Leave 1 core free for UI updates\n",
    "VERBOSE=2 # 3 is the most verbose level\n",
    "NB_EPOCH = 50 # todo ! Check how many epochs in the paper\n",
    "BATCH = 256 # todo ! Check this in the paper too\n",
    "\n",
    "\n",
    "def stratified_subsample(X, y, num_rows):\n",
    "    '''Creates a stratified subsample of X and y\n",
    "    INPUT: X and y, numpy arrays\n",
    "    RETURNS: subset of X and y, maintaining class balances\n",
    "    '''\n",
    "    # Create a stratified, shuffled subset of the training data if needed\n",
    "    N = X.shape[0]\n",
    "    if num_rows < N:\n",
    "        print('Reducing size from {} to {} examples'.format(N, num_rows))\n",
    "        new_X, _, new_y, _ = train_test_split(X_train, y_train, \n",
    "                                              train_size=N, random_state=SEED)    \n",
    "\n",
    "def onehot_encode_y(y_train, y_test):\n",
    "    '''Convert y_train and y_test to a one-hot encoding version\n",
    "    INPUT: y_train - np.array of size (n_train,)\n",
    "           y_test - np.array of size (n_test,)\n",
    "    RETURNS: y_train - np.array of size (n_train, n_classes)\n",
    "             y_test - np.arary of size (n_test, n_classes)\n",
    "    '''    \n",
    "    print('Converting y variables to one-hot encoding..')\n",
    "    lbe = LabelBinarizer()\n",
    "    lbe.fit(y_train)\n",
    "    y_train = lbe.transform(y_train)\n",
    "    y_test = lbe.transform(y_test)\n",
    "    return y_train, y_test\n",
    "\n",
    "def z_norm_X(X_train, X_test):\n",
    "    '''Z-normalizes X_train and X_test with 0 mean and 1 std. dev.\n",
    "    INPUT: X_train - training set\n",
    "           X_test - test set\n",
    "    RETURNS: X_train - normalized version of same size\n",
    "             X_test - normalized version (using X_train parameters)\n",
    "    '''\n",
    "    print('Z-normalizing X data..')    \n",
    "    std = StandardScaler()\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    std.fit(X_train)\n",
    "    X_train = std.transform(X_train)\n",
    "    X_test = std.transform(X_test)\n",
    "    return X_train, X_test\n",
    "    \n",
    "y_train, y_test = onehot_encode_y(y_train, y_test)\n",
    "X_train, X_test = z_norm_X(X_train, X_test)\n",
    "\n",
    "print('Train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.5 - One hidden layer models\n",
    "\n",
    "The paper continues with an evaluation of single hidden-layer models. We'll be training a selection of three models from the paper, whhich are listed below.  All use sigmoid activations.\n",
    "\n",
    "* Model a - 28x28-300-10: 4.7% Error\n",
    "* Model b - 20x20-300-10:  1.6% Error (images were reduced to 20x20 and centred in 28x28  background)\n",
    "* Model c - 28x28-1000-10: 4.5% Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model a\n",
    "\n",
    "This is a baseline FC network which should give 4.7% error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1070 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 300)           235500      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 300)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            3010        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 10)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 238510\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "Training model\n",
      "\n",
      "Epoch 1/50\n",
      "0s - loss: 0.3661 - acc: 0.8923\n",
      "Epoch 2/50\n",
      "0s - loss: 0.2124 - acc: 0.9378\n",
      "Epoch 3/50\n",
      "0s - loss: 0.1664 - acc: 0.9527\n",
      "Epoch 4/50\n",
      "0s - loss: 0.1342 - acc: 0.9614\n",
      "Epoch 5/50\n",
      "0s - loss: 0.1101 - acc: 0.9691\n",
      "Epoch 6/50\n",
      "0s - loss: 0.0919 - acc: 0.9747\n",
      "Epoch 7/50\n",
      "0s - loss: 0.0769 - acc: 0.9795\n",
      "Epoch 8/50\n",
      "0s - loss: 0.0651 - acc: 0.9827\n",
      "Epoch 9/50\n",
      "0s - loss: 0.0551 - acc: 0.9860\n",
      "Epoch 10/50\n",
      "0s - loss: 0.0471 - acc: 0.9881\n",
      "Epoch 11/50\n",
      "0s - loss: 0.0402 - acc: 0.9906\n",
      "Epoch 12/50\n",
      "0s - loss: 0.0351 - acc: 0.9923\n",
      "Epoch 13/50\n",
      "0s - loss: 0.0300 - acc: 0.9937\n",
      "Epoch 14/50\n",
      "0s - loss: 0.0262 - acc: 0.9950\n",
      "Epoch 15/50\n",
      "0s - loss: 0.0229 - acc: 0.9959\n",
      "Epoch 16/50\n",
      "0s - loss: 0.0201 - acc: 0.9968\n",
      "Epoch 17/50\n",
      "0s - loss: 0.0178 - acc: 0.9976\n",
      "Epoch 18/50\n",
      "0s - loss: 0.0158 - acc: 0.9981\n",
      "Epoch 19/50\n",
      "0s - loss: 0.0142 - acc: 0.9985\n",
      "Epoch 20/50\n",
      "0s - loss: 0.0129 - acc: 0.9986\n",
      "Epoch 21/50\n",
      "0s - loss: 0.0116 - acc: 0.9990\n",
      "Epoch 22/50\n",
      "0s - loss: 0.0105 - acc: 0.9992\n",
      "Epoch 23/50\n",
      "0s - loss: 0.0096 - acc: 0.9993\n",
      "Epoch 24/50\n",
      "0s - loss: 0.0088 - acc: 0.9996\n",
      "Epoch 25/50\n",
      "0s - loss: 0.0081 - acc: 0.9996\n",
      "Epoch 26/50\n",
      "0s - loss: 0.0075 - acc: 0.9997\n",
      "Epoch 27/50\n",
      "0s - loss: 0.0070 - acc: 0.9997\n",
      "Epoch 28/50\n",
      "0s - loss: 0.0065 - acc: 0.9998\n",
      "Epoch 29/50\n",
      "0s - loss: 0.0060 - acc: 0.9998\n",
      "Epoch 30/50\n",
      "0s - loss: 0.0057 - acc: 0.9998\n",
      "Epoch 31/50\n",
      "0s - loss: 0.0053 - acc: 0.9999\n",
      "Epoch 32/50\n",
      "0s - loss: 0.0050 - acc: 0.9999\n",
      "Epoch 33/50\n",
      "0s - loss: 0.0048 - acc: 0.9999\n",
      "Epoch 34/50\n",
      "0s - loss: 0.0045 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "0s - loss: 0.0043 - acc: 0.9999\n",
      "Epoch 36/50\n",
      "0s - loss: 0.0041 - acc: 0.9999\n",
      "Epoch 37/50\n",
      "0s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "0s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "0s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "0s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "0s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "0s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "0s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "0s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "0s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "0s - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "0s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "0s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "0s - loss: 0.0024 - acc: 1.0000\n",
      "\n",
      "Generating predictions on test set\n",
      "\n",
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "\n",
      "Test set training error 0.8925, test error 0.0267\n"
     ]
    }
   ],
   "source": [
    "# Model a - 28x28-300-10: 4.7% Error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "print('Model summary:\\n')\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining model\\n')\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=NB_EPOCH,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2)\n",
    "\n",
    "print('\\nGenerating predictions on test set\\n')\n",
    "scores = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "errors = [1.0 - score for score in scores]\n",
    "\n",
    "print('\\n\\nTest set training error {:.4f}, test error {:.4f}'.format(errors[0], errors[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model b \n",
    "\n",
    "This model requires some preprocessing of the input data, to shrink the images into a 20x20 area centred in a 28x28 image. After that we need to remember to Z-Normalize them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check a few training values at random as a sanity check\n",
    "def show_label_images(X, y, images=None):\n",
    "    '''Shows random images in a grid\n",
    "    INPUT: X - image data\n",
    "           y - class label\n",
    "           images - indexes of images to show. Randomly selected if None\n",
    "    RETURNS: Nothing.\n",
    "    '''\n",
    "    \n",
    "    num = 4\n",
    "    num_square = num ** 2\n",
    "    \n",
    "    if images is None:\n",
    "        images = np.random.randint(0, n_train, num_square)\n",
    "        \n",
    "    print('Showing training image indexes {}'.format(images))\n",
    "\n",
    "    fig, axes = plt.subplots(num,num, figsize=(6,6))\n",
    "    for idx, val in enumerate(images):\n",
    "        r, c = divmod(idx, num)\n",
    "        ax = axes[r][c]\n",
    "        ax.imshow(X[images[idx]], cmap=plt.cm.binary)\n",
    "        ax.annotate('Label: {}'.format(y[val]), xy=(1, 1))\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "show_label_images(X_train_input, y_train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import *\n",
    "\n",
    "def resize_image(image, img_size, back_size, back_color):\n",
    "    '''Resizes the image to `new_size`, pastes in center of backgrond\n",
    "    INPUT: image = Input image\n",
    "           img_size = tuple of images new dimensions\n",
    "           back_size = tuple of new image background dimensions\n",
    "           back_color = color to fill background with\n",
    "    RETURNS: New image\n",
    "    '''\n",
    "\n",
    "    new_img = Image.new('L', back_size, (back_color,))\n",
    "    img = Image.fromarray(image)\n",
    "    img.thumbnail(img_size, Image.ANTIALIAS) \n",
    "\n",
    "    h_diff = int((back_size[0] - img_size[1]) / 2)\n",
    "    w_diff = int((back_size[1] - img_size[1]) / 2)\n",
    "\n",
    "    new_img.paste(img, (h_diff, w_diff))\n",
    "    return np.asarray(new_img)\n",
    "\n",
    "def resize_image_array(X, img_size, back_size, back_color, desc=None):\n",
    "    '''Resizes images held in a numpy array'''\n",
    "    X_out = np.zeros_like(X)\n",
    "    n_images = X.shape[0]\n",
    "    for idx in tqdm(range(n_images), desc=desc): # \"Resizing {} images\".format(n_images)):\n",
    "        X_out[idx] = resize_image(X[idx], img_size, back_size, 0)\n",
    "        \n",
    "    return X_out\n",
    "\n",
    "X_train_resize = resize_image_array(X_train_input, (20,20), (28,28), 0, 'Resizing training images')\n",
    "X_test_resize = resize_image_array(X_test_input, (20,20), (28,28), 0, 'Resizing training images')\n",
    "\n",
    "# Reshape the images so they're a single row in the numpy array\n",
    "X_train_resize = X_train_resize_float.reshape((n_train, w * h))\n",
    "X_test_resize = X_test_resize_float.reshape((n_test, w * h))\n",
    "\n",
    "\n",
    "print('Z-normalizing X data')\n",
    "std = StandardScaler()\n",
    "X_train_resize_float = X_train_resize.astype(np.float32)\n",
    "X_test_resize_float = X_test_resize.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "std.fit(X_train_resize_float.astype(np.float32))\n",
    "X_train = std.transform(X_train_resize_float)\n",
    "X_test = std.transform(X_test_resize_float)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model a - 28x28-300-10: 4.7% Error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "print('Model summary:\\n')\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining model\\n')\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=NB_EPOCH,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2)\n",
    "\n",
    "print('\\nGenerating predictions on test set\\n')\n",
    "scores = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "errors = [1.0 - score for score in scores]\n",
    "\n",
    "print('\\n\\nTest set training error {:.4f}, test error {:.4f}'.format(errors[0], errors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
