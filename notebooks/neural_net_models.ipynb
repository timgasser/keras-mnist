{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models\n",
    "\n",
    "This notebook picks up after the `simple_models` notebook. After trying a range of classification algorithms, we'll try out some of the neural network models in [1]. These include fully-connected models of varying layer sizes, and finally convolutional models including the famous LeNet-5. \n",
    "\n",
    "Along the way, we'll be using Keras which is a library sitting on top of Theano or Tensorflow. This allows easy construction, training and evaluation of neural nets. Before we get started, here's a recap of the `simple_models` notebook models.\n",
    "\n",
    "`[1]` - [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf),  LeCun et al, Nov 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in dependencies, may not use all of these\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle files ...\n",
      "Loaded train images shape (60000, 28, 28), labels shape (60000, 1)\n",
      "Loaded test images shape (10000, 28, 28), labels shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set up the file directory and names\n",
    "DIR = '../input/'\n",
    "X_TRAIN = DIR + 'train-images-idx3-ubyte.pkl'\n",
    "Y_TRAIN = DIR + 'train-labels-idx1-ubyte.pkl'\n",
    "X_TEST = DIR + 't10k-images-idx3-ubyte.pkl'\n",
    "Y_TEST = DIR + 't10k-labels-idx1-ubyte.pkl'\n",
    "\n",
    "print('Loading pickle files ...')\n",
    "X_train = pickle.load( open( X_TRAIN, \"rb\" ) )\n",
    "y_train = pickle.load( open( Y_TRAIN, \"rb\" ) )\n",
    "X_test = pickle.load( open( X_TEST, \"rb\" ) )\n",
    "y_test = pickle.load( open( Y_TEST, \"rb\" ) )\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "w = X_train.shape[1]\n",
    "h = X_train.shape[2]\n",
    "\n",
    "# Reshape the images so they're a single row in the numpy array\n",
    "X_train_input = X_train.reshape((n_train, w * h))\n",
    "X_test_input = X_test.reshape((n_test, w * h))\n",
    "y_train_input = y_train.squeeze()\n",
    "y_test_input = y_test.squeeze()\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "print('Loaded train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Loaded test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Before trying a few different algorithms out, let's define a reusable set of functions to cross-validate and predict on the test set. Because scikit-learn has such a uniform interface, we can re-use these on pretty much any classification algorithm out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# todo !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Before we start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting y variables to one-hot encoding\n",
      "Z-normalizing X\n",
      "Train images shape (60000, 784), labels shape (60000, 10)\n",
      "Test images shape (10000, 784), labels shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "\n",
    "N_JOBS=-2 # Leave 1 core free for UI updates\n",
    "VERBOSE=1 # 3 is the most verbose level\n",
    "SEED = 1234 # Fix the seed for repeatability\n",
    "MAX_ITER = 100 # L-BFGS may show warnings that it doesn't converge\n",
    "N_FOLDS = 5 # How may folds to do k-folds cross validation for\n",
    "\n",
    "# Create a stratified, shuffled subset of the training data if needed\n",
    "N = n_train # How may training examples to use\n",
    "if N < n_train:\n",
    "    print('Reducing the X_train size from {} to {} examples'.format(n_train, N))\n",
    "    X_train, _, y_train, _ = train_test_split(X_train_input, y_train_input, \n",
    "                                          train_size=N, random_state=SEED)\n",
    "else:\n",
    "    X_train = X_train_input\n",
    "    y_train = y_train_input\n",
    "    \n",
    "X_test = X_test_input\n",
    "y_test = y_test_input\n",
    "\n",
    "\n",
    "# Need to convert the classes to one-hot encoding\n",
    "print('Converting y variables to one-hot encoding')\n",
    "lbe = LabelBinarizer()\n",
    "lbe.fit(y_train_input)\n",
    "y_train = lbe.transform(y_train_input)\n",
    "y_test = lbe.transform(y_test_input)\n",
    "\n",
    "print('Z-normalizing X')\n",
    "std = StandardScaler()\n",
    "X_train_float = X_train.astype(np.float32)\n",
    "X_test_float = X_test.astype(np.float32)\n",
    "std.fit(X_train.astype(np.float32))\n",
    "X_train = std.transform(X_train_float)\n",
    "X_test = std.transform(X_test_float)\n",
    "\n",
    "print('Train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.5 - One hidden layer models\n",
    "\n",
    "The paper continues with an evaluation of single hidden-layer models. We'll be training a selection of three models from the paper, whhich are listed below.  All use sigmoid activations.\n",
    "\n",
    "* Model a - 28x28-300-10: 4.7% Error\n",
    "* Model b - 20x20-300-10:  1.6% Error (images were reduced to 20x20 and centred in 28x28  background)\n",
    "* Model c - 28x28-1000-10: 4.5% Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "0s - loss: 0.3653 - acc: 0.8926\n",
      "Epoch 2/20\n",
      "0s - loss: 0.2123 - acc: 0.9387\n",
      "Epoch 3/20\n",
      "0s - loss: 0.1659 - acc: 0.9529\n",
      "Epoch 4/20\n",
      "0s - loss: 0.1326 - acc: 0.9627\n",
      "Epoch 5/20\n",
      "0s - loss: 0.1089 - acc: 0.9698\n",
      "Epoch 6/20\n",
      "0s - loss: 0.0900 - acc: 0.9751\n",
      "Epoch 7/20\n",
      "0s - loss: 0.0750 - acc: 0.9796\n",
      "Epoch 8/20\n",
      "0s - loss: 0.0633 - acc: 0.9830\n",
      "Epoch 9/20\n",
      "0s - loss: 0.0536 - acc: 0.9861\n",
      "Epoch 10/20\n",
      "0s - loss: 0.0457 - acc: 0.9888\n",
      "Epoch 11/20\n",
      "0s - loss: 0.0393 - acc: 0.9908\n",
      "Epoch 12/20\n",
      "0s - loss: 0.0336 - acc: 0.9928\n",
      "Epoch 13/20\n",
      "0s - loss: 0.0290 - acc: 0.9942\n",
      "Epoch 14/20\n",
      "0s - loss: 0.0253 - acc: 0.9952\n",
      "Epoch 15/20\n",
      "0s - loss: 0.0223 - acc: 0.9963\n",
      "Epoch 16/20\n",
      "0s - loss: 0.0194 - acc: 0.9971\n",
      "Epoch 17/20\n",
      "0s - loss: 0.0173 - acc: 0.9975\n",
      "Epoch 18/20\n",
      "0s - loss: 0.0154 - acc: 0.9980\n",
      "Epoch 19/20\n",
      "0s - loss: 0.0138 - acc: 0.9985\n",
      "Epoch 20/20\n",
      "0s - loss: 0.0125 - acc: 0.9989\n",
      " 8448/10000 [========================>.....] - ETA: 0s[0.10051630364619195, 0.9728]\n"
     ]
    }
   ],
   "source": [
    "## from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=20,\n",
    "          batch_size=256,\n",
    "          verbose=2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2s - loss: 8.0939 - acc: 0.4733\n",
      "Epoch 2/20\n",
      "2s - loss: 8.0675 - acc: 0.4871\n",
      "Epoch 3/20\n",
      "2s - loss: 8.0477 - acc: 0.4922\n",
      "Epoch 4/20\n",
      "2s - loss: 8.0349 - acc: 0.4953\n",
      "Epoch 5/20\n",
      "2s - loss: 8.0282 - acc: 0.4969\n",
      "Epoch 6/20\n",
      "2s - loss: 8.0213 - acc: 0.4988\n",
      "Epoch 7/20\n",
      "2s - loss: 8.0168 - acc: 0.5003\n",
      "Epoch 8/20\n",
      "2s - loss: 8.0132 - acc: 0.5012\n",
      "Epoch 9/20\n",
      "2s - loss: 8.0107 - acc: 0.5022\n",
      "Epoch 10/20\n",
      "2s - loss: 8.0087 - acc: 0.5030\n",
      "Epoch 11/20\n",
      "2s - loss: 8.0081 - acc: 0.5031\n",
      "Epoch 12/20\n",
      "2s - loss: 8.0080 - acc: 0.5031\n",
      "Epoch 13/20\n",
      "2s - loss: 8.0077 - acc: 0.5031\n",
      "Epoch 14/20\n",
      "2s - loss: 8.0075 - acc: 0.5032\n",
      "Epoch 15/20\n",
      "2s - loss: 8.0075 - acc: 0.5032\n",
      "Epoch 16/20\n",
      "2s - loss: 8.0075 - acc: 0.5032\n",
      "Epoch 17/20\n",
      "2s - loss: 8.0074 - acc: 0.5032\n",
      "Epoch 18/20\n",
      "2s - loss: 8.0074 - acc: 0.5032\n",
      "Epoch 19/20\n",
      "2s - loss: 8.0074 - acc: 0.5032\n",
      "Epoch 20/20\n",
      "2s - loss: 8.0073 - acc: 0.5032\n",
      " 9888/10000 [============================>.] - ETA: 0s[8.0899054382324227, 0.49249999999999999]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=20,\n",
    "          batch_size=16,\n",
    "          verbose=2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
