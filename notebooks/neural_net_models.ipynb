{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models\n",
    "\n",
    "This notebook picks up after the `simple_models` notebook. After trying a range of classification algorithms, we'll try out some of the neural network models in [1]. These include fully-connected models of varying layer sizes, and finally convolutional models including the famous LeNet-5. \n",
    "\n",
    "Along the way, we'll be using Keras which is a library sitting on top of Theano or Tensorflow. This allows easy construction, training and evaluation of neural nets. Before we get started, here's a recap of the `simple_models` notebook models.\n",
    "\n",
    "`[1]` - [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf),  LeCun et al, Nov 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in dependencies, may not use all of these\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pickle files\n",
    "\n",
    "The original data files are processed using the `convert_data.py` script, and written out to pickle files. We can load these in as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle files ...\n"
     ]
    }
   ],
   "source": [
    "# Set up the file directory and names\n",
    "DIR = '../input/'\n",
    "X_TRAIN = DIR + 'train-images-idx3-ubyte.pkl'\n",
    "Y_TRAIN = DIR + 'train-labels-idx1-ubyte.pkl'\n",
    "X_TEST = DIR + 't10k-images-idx3-ubyte.pkl'\n",
    "Y_TEST = DIR + 't10k-labels-idx1-ubyte.pkl'\n",
    "\n",
    "def load_data():\n",
    "    '''Loads pickled ubyte files with MNIST data\n",
    "    INPUT: X_train_file, y_train_file - strings with training filenames\n",
    "           X_test_file, y_test_File - strings with test filenames\n",
    "    RETURNS: Tuple with (X_train, y_train, X_test, y_test)\n",
    "    '''\n",
    "    print('Loading pickle files ...')\n",
    "    try:\n",
    "        X_train = pickle.load( open( X_TRAIN, \"rb\" ) )\n",
    "        y_train = pickle.load( open( Y_TRAIN, \"rb\" ) )\n",
    "        X_test = pickle.load( open( X_TEST, \"rb\" ) )\n",
    "        y_test = pickle.load( open( Y_TEST, \"rb\" ) )\n",
    "    except:\n",
    "        print('Error loading pickle file')\n",
    "        return None\n",
    "        \n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "X_train, y_train, X_test,  y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Before evaluating some models on the images, let's create some helper functions we can re-use later on. These deal with converting images to and from 1d and 2d versions, plotting images, resizing them, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train images shape (60000, 784), labels shape (60000, 1)\n",
      "Loaded test images shape (10000, 784), labels shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "def flatten_images(X):\n",
    "    ''' Converts images to 1-d vectors\n",
    "    INPUT: X - Input array of shape [n, w, h]\n",
    "    RETURNS: Numpy array of shape [n, w*h]\n",
    "    '''\n",
    "    n, w, h = X.shape\n",
    "    X_flat = X.reshape((n, w * h))\n",
    "    return X_flat\n",
    "\n",
    "def square_images(X, w, h):\n",
    "    '''Converts single-vector images into square images \n",
    "    INPUT: X - numpy array of images in single-vector form\n",
    "           w - width of images to convert to\n",
    "           h - height of images to convert to\n",
    "    RETURNS: Numpy array of shape [n, w, h]\n",
    "    '''\n",
    "    assert X.shape[1] == w * h, \"Error - Can't square array of shape {} to {}\".format(X.shape, (w, h))\n",
    "    n = X.shape[0]\n",
    "    X_square = X.reshape((n, w, h))\n",
    "    return X_square\n",
    "\n",
    "\n",
    "N_TRAIN, W, H = X_train.shape\n",
    "N_TEST, w_test, h_test = X_test.shape\n",
    "\n",
    "# Flatten the images\n",
    "X_train = flatten_images(X_train)\n",
    "X_test = flatten_images(X_test)\n",
    "\n",
    "# Do some checks on the data\n",
    "assert N_TRAIN == 60000, 'Error - expected 60000 training images, got {}'.format(N_TRAIN)\n",
    "assert N_TEST == 10000, 'Error - expected 60000 training images, got {}'.format(N_TEST)\n",
    "assert W == w_test, 'Error - width mismatch. Train {}, Test {}'.format(w, w_test)\n",
    "assert H == h_test, 'Error - height mismatch. Train {}, Test {}'.format(h, h_test)\n",
    "\n",
    "assert np.array_equal(X_train, flatten_images(square_images(X_train, W, H)))\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "print('Loaded train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Loaded test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "This section sets up global constants used in all models (to ensure a fair comparison). It also prepares the data by converting y values to one-hot, and normalizing X inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting y variables to one-hot encoding..\n",
      "Z-normalizing X data..\n",
      "Train images shape (60000, 784), labels shape (60000, 10)\n",
      "Test images shape (10000, 784), labels shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "\n",
    "# Keras Common configuration\n",
    "SEED = 1234 # Fix the seed for repeatability\n",
    "N_JOBS=-2 # Leave 1 core free for UI updates\n",
    "VERBOSE=2 # 3 is the most verbose level\n",
    "NB_EPOCH = 20 # todo ! Check how many epochs in the paper\n",
    "BATCH = 256 # todo ! Check this in the paper too\n",
    "\n",
    "\n",
    "def stratified_subsample(X, y, num_rows):\n",
    "    '''Creates a stratified subsample of X and y\n",
    "    INPUT: X and y, numpy arrays\n",
    "    RETURNS: subset of X and y, maintaining class balances\n",
    "    '''\n",
    "    # Create a stratified, shuffled subset of the training data if needed\n",
    "    N = X.shape[0]\n",
    "    if num_rows < N:\n",
    "        print('Reducing size from {} to {} examples'.format(N, num_rows))\n",
    "        new_X, _, new_y, _ = train_test_split(X_train, y_train, \n",
    "                                              train_size=N, random_state=SEED)    \n",
    "\n",
    "def onehot_encode_y(y_train, y_test):\n",
    "    '''Convert y_train and y_test to a one-hot encoding version\n",
    "    INPUT: y_train - np.array of size (n_train,)\n",
    "           y_test - np.array of size (n_test,)\n",
    "    RETURNS: y_train - np.array of size (n_train, n_classes)\n",
    "             y_test - np.arary of size (n_test, n_classes)\n",
    "    '''    \n",
    "    print('Converting y variables to one-hot encoding..')\n",
    "    lbe = LabelBinarizer()\n",
    "    lbe.fit(y_train)\n",
    "    y_train = lbe.transform(y_train)\n",
    "    y_test = lbe.transform(y_test)\n",
    "    return y_train, y_test\n",
    "\n",
    "def z_norm_X(X_train, X_test):\n",
    "    '''Z-normalizes X_train and X_test with 0 mean and 1 std. dev.\n",
    "    INPUT: X_train - training set\n",
    "           X_test - test set\n",
    "    RETURNS: X_train - normalized version of same size\n",
    "             X_test - normalized version (using X_train parameters)\n",
    "    '''\n",
    "    print('Z-normalizing X data..')    \n",
    "    std = StandardScaler()\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    std.fit(X_train)\n",
    "    X_train = std.transform(X_train)\n",
    "    X_test = std.transform(X_test)\n",
    "    return X_train, X_test\n",
    "    \n",
    "y_train, y_test = onehot_encode_y(y_train, y_test)\n",
    "X_train, X_test = z_norm_X(X_train, X_test)\n",
    "scores = dict()\n",
    "\n",
    "print('Train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.5 - One hidden layer models\n",
    "\n",
    "The paper continues with an evaluation of single hidden-layer models. We'll be training a selection of three models from the paper, whhich are listed below.  All use sigmoid activations.\n",
    "\n",
    "* Model a - 28x28-300-10: 4.7% Error\n",
    "* Model b - 20x20-300-10:  1.6% Error (images were reduced to 20x20 and centred in 28x28  background)\n",
    "* Model c - 28x28-1000-10: 4.5% Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model a\n",
    "\n",
    "This is a baseline FC network which should give 4.7% error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_28 (Dense)                 (None, 300)           235500      dense_input_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 300)           0           dense_28[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 10)            3010        activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 10)            0           dense_29[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 238510\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "Training model\n",
      "\n",
      "Epoch 1/20\n",
      "1s - loss: 0.3642 - acc: 0.8929\n",
      "Epoch 2/20\n",
      "0s - loss: 0.2102 - acc: 0.9393\n",
      "Epoch 3/20\n",
      "0s - loss: 0.1651 - acc: 0.9528\n",
      "Epoch 4/20\n",
      "0s - loss: 0.1332 - acc: 0.9618\n",
      "Epoch 5/20\n",
      "0s - loss: 0.1092 - acc: 0.9695\n",
      "Epoch 6/20\n",
      "0s - loss: 0.0902 - acc: 0.9750\n",
      "Epoch 7/20\n",
      "0s - loss: 0.0757 - acc: 0.9799\n",
      "Epoch 8/20\n",
      "0s - loss: 0.0637 - acc: 0.9831\n",
      "Epoch 9/20\n",
      "0s - loss: 0.0541 - acc: 0.9862\n",
      "Epoch 10/20\n",
      "0s - loss: 0.0461 - acc: 0.9889\n",
      "Epoch 11/20\n",
      "0s - loss: 0.0396 - acc: 0.9910\n",
      "Epoch 12/20\n",
      "0s - loss: 0.0342 - acc: 0.9923\n",
      "Epoch 13/20\n",
      "0s - loss: 0.0298 - acc: 0.9939\n",
      "Epoch 14/20\n",
      "0s - loss: 0.0258 - acc: 0.9950\n",
      "Epoch 15/20\n",
      "0s - loss: 0.0228 - acc: 0.9960\n",
      "Epoch 16/20\n",
      "0s - loss: 0.0200 - acc: 0.9970\n",
      "Epoch 17/20\n",
      "0s - loss: 0.0176 - acc: 0.9976\n",
      "Epoch 18/20\n",
      "0s - loss: 0.0158 - acc: 0.9980\n",
      "Epoch 19/20\n",
      "0s - loss: 0.0141 - acc: 0.9985\n",
      "Epoch 20/20\n",
      "0s - loss: 0.0128 - acc: 0.9987\n",
      "\n",
      "Generating predictions on test set\n",
      "\n",
      " 9728/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Test set loss 0.1001, error 0.0270\n"
     ]
    }
   ],
   "source": [
    "# Model a - 28x28-300-10: 4.7% Error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "class keras_model(object):\n",
    "    \n",
    "    def __init__(model_type, input_dim, layers, activation, output_activation, verbose=None):\n",
    "        '''Initializes a new keras model'''\n",
    "        model = model_type()\n",
    "        for idx, size in enumerate(layers):\n",
    "            if idx == 0:\n",
    "                model.add(Dense(size, input_dim))\n",
    "            else if idx == len(layers) - 1:\n",
    "                model.add(Dense(size))\n",
    "                model.add(output_activation)\n",
    "            else:\n",
    "\n",
    "        if verbose is not None:\n",
    "            print('Model summary:\\n')\n",
    "            model.summary()\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def add_optimizer(optim):\n",
    "        '''Adds the optimizer to the class'''\n",
    "        self.optimizer = optim\n",
    "        \n",
    "    def compile():\n",
    "        \n",
    "    def fit(X, y)\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining model\\n')\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=NB_EPOCH,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2)\n",
    "\n",
    "print('\\nGenerating predictions on test set\\n')\n",
    "result = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "error = 1.0 - scores[1]\n",
    "scores['fc-300-10'] = error\n",
    "\n",
    "print('\\n\\nTest set softmax loss {:.4f}, error {:.4f}'.format(scores[0], 1.0 - scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected 1000-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_21 (Dense)                 (None, 1000)          785000      dense_input_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 1000)          0           dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 10)            10010       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 10)            0           dense_22[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 795010\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "Training model\n",
      "\n",
      "Epoch 1/20\n",
      "2s - loss: 4.9370 - acc: 0.6329\n",
      "Epoch 2/20\n",
      "2s - loss: 4.8649 - acc: 0.6663\n",
      "Epoch 3/20\n",
      "2s - loss: 4.8481 - acc: 0.6717\n",
      "Epoch 4/20\n",
      "2s - loss: 4.8292 - acc: 0.6771\n",
      "Epoch 5/20\n",
      "2s - loss: 4.8140 - acc: 0.6823\n",
      "Epoch 6/20\n",
      "2s - loss: 4.8009 - acc: 0.6855\n",
      "Epoch 7/20\n",
      "2s - loss: 4.7898 - acc: 0.6893\n",
      "Epoch 8/20\n",
      "2s - loss: 4.7799 - acc: 0.6924\n",
      "Epoch 9/20\n",
      "2s - loss: 4.7728 - acc: 0.6947\n",
      "Epoch 10/20\n",
      "2s - loss: 4.7666 - acc: 0.6964\n",
      "Epoch 11/20\n",
      "2s - loss: 4.7617 - acc: 0.6981\n",
      "Epoch 12/20\n",
      "2s - loss: 4.7573 - acc: 0.6995\n",
      "Epoch 13/20\n",
      "2s - loss: 4.7537 - acc: 0.7008\n",
      "Epoch 14/20\n",
      "2s - loss: 4.7507 - acc: 0.7022\n",
      "Epoch 15/20\n",
      "2s - loss: 4.7485 - acc: 0.7027\n",
      "Epoch 16/20\n",
      "2s - loss: 4.7467 - acc: 0.7032\n",
      "Epoch 17/20\n",
      "2s - loss: 4.7448 - acc: 0.7041\n",
      "Epoch 18/20\n",
      "2s - loss: 4.7434 - acc: 0.7045\n",
      "Epoch 19/20\n",
      "2s - loss: 4.7422 - acc: 0.7045\n",
      "Epoch 20/20\n",
      "2s - loss: 4.7411 - acc: 0.7049\n",
      "\n",
      "Generating predictions on test set\n",
      "\n",
      " 9728/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Test set training error -3.7975, test error 0.3097\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "print('Model summary:\\n')\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining model\\n')\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=NB_EPOCH,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2)\n",
    "\n",
    "print('\\nGenerating predictions on test set\\n')\n",
    "scores = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "errors = [1.0 - score for score in scores]\n",
    "\n",
    "print('\\n\\nTest set training error {:.4f}, test error {:.4f}'.format(errors[0], errors[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected 300-100-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_23 (Dense)                 (None, 300)           235500      dense_input_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 300)           0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 100)           30100       activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 100)           0           dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 10)            1010        activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 10)            0           dense_25[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 266610\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "Training model\n",
      "\n",
      "Epoch 1/20\n",
      "1s - loss: 0.5301 - acc: 0.8523\n",
      "Epoch 2/20\n",
      "1s - loss: 0.2238 - acc: 0.9337\n",
      "Epoch 3/20\n",
      "1s - loss: 0.1768 - acc: 0.9483\n",
      "Epoch 4/20\n",
      "1s - loss: 0.1455 - acc: 0.9574\n",
      "Epoch 5/20\n",
      "1s - loss: 0.1206 - acc: 0.9649\n",
      "Epoch 6/20\n",
      "1s - loss: 0.1017 - acc: 0.9708\n",
      "Epoch 7/20\n",
      "1s - loss: 0.0857 - acc: 0.9757\n",
      "Epoch 8/20\n",
      "1s - loss: 0.0719 - acc: 0.9796\n",
      "Epoch 9/20\n",
      "1s - loss: 0.0611 - acc: 0.9833\n",
      "Epoch 10/20\n",
      "1s - loss: 0.0518 - acc: 0.9866\n",
      "Epoch 11/20\n",
      "1s - loss: 0.0434 - acc: 0.9891\n",
      "Epoch 12/20\n",
      "1s - loss: 0.0366 - acc: 0.9911\n",
      "Epoch 13/20\n",
      "1s - loss: 0.0305 - acc: 0.9929\n",
      "Epoch 14/20\n",
      "1s - loss: 0.0255 - acc: 0.9948\n",
      "Epoch 15/20\n",
      "1s - loss: 0.0214 - acc: 0.9960\n",
      "Epoch 16/20\n",
      "1s - loss: 0.0181 - acc: 0.9968\n",
      "Epoch 17/20\n",
      "1s - loss: 0.0152 - acc: 0.9977\n",
      "Epoch 18/20\n",
      "1s - loss: 0.0130 - acc: 0.9983\n",
      "Epoch 19/20\n",
      "1s - loss: 0.0110 - acc: 0.9987\n",
      "Epoch 20/20\n",
      "1s - loss: 0.0096 - acc: 0.9990\n",
      "\n",
      "Generating predictions on test set\n",
      "\n",
      " 9728/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Test set training error 0.9022, test error 0.0278\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "print('Model summary:\\n')\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining model\\n')\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=NB_EPOCH,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2)\n",
    "\n",
    "print('\\nGenerating predictions on test set\\n')\n",
    "scores = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "errors = [1.0 - score for score in scores]\n",
    "\n",
    "print('\\n\\nTest set training error {:.4f}, test error {:.4f}'.format(errors[0], errors[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected 500-150-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_30 (Dense)                 (None, 500)           392500      dense_input_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 500)           0           dense_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 150)           75150       activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 150)           0           dense_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 10)            1510        activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 10)            0           dense_32[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 469160\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "Training model\n",
      "\n",
      "Epoch 1/20\n",
      "1s - loss: 0.5027 - acc: 0.8579\n",
      "Epoch 2/20\n",
      "1s - loss: 0.2287 - acc: 0.9322\n",
      "Epoch 3/20\n",
      "1s - loss: 0.1818 - acc: 0.9474\n",
      "Epoch 4/20\n",
      "1s - loss: 0.1502 - acc: 0.9560\n",
      "Epoch 5/20\n",
      "1s - loss: 0.1264 - acc: 0.9625\n",
      "Epoch 6/20\n",
      "1s - loss: 0.1076 - acc: 0.9690\n",
      "Epoch 7/20\n",
      "1s - loss: 0.0925 - acc: 0.9729\n",
      "Epoch 8/20\n",
      "1s - loss: 0.0788 - acc: 0.9773\n",
      "Epoch 9/20\n",
      "1s - loss: 0.0677 - acc: 0.9806\n",
      "Epoch 10/20\n",
      "1s - loss: 0.0585 - acc: 0.9838\n",
      "Epoch 11/20\n",
      "1s - loss: 0.0500 - acc: 0.9862\n",
      "Epoch 12/20\n",
      "1s - loss: 0.0426 - acc: 0.9888\n",
      "Epoch 13/20\n",
      "1s - loss: 0.0366 - acc: 0.9909\n",
      "Epoch 14/20\n",
      "1s - loss: 0.0309 - acc: 0.9926\n",
      "Epoch 15/20\n",
      "1s - loss: 0.0263 - acc: 0.9940\n",
      "Epoch 16/20\n",
      "1s - loss: 0.0228 - acc: 0.9950\n",
      "Epoch 17/20\n",
      "1s - loss: 0.0193 - acc: 0.9961\n",
      "Epoch 18/20\n",
      "1s - loss: 0.0162 - acc: 0.9974\n",
      "Epoch 19/20\n",
      "1s - loss: 0.0140 - acc: 0.9979\n",
      "Epoch 20/20\n",
      "1s - loss: 0.0120 - acc: 0.9983\n",
      "\n",
      "Generating predictions on test set\n",
      "\n",
      " 9728/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Test set training error 0.9022, test error 0.0259\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(150))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "print('Model summary:\\n')\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining model\\n')\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=NB_EPOCH,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2)\n",
    "\n",
    "print('\\nGenerating predictions on test set\\n')\n",
    "scores = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "errors = [1.0 - score for score in scores]\n",
    "\n",
    "print('\\n\\nTest set training error {:.4f}, test error {:.4f}'.format(errors[0], errors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check a few training values at random as a sanity check\n",
    "def show_label_images(X, y, images=None):\n",
    "    '''Shows random images in a grid\n",
    "    INPUT: X - image data\n",
    "           y - class label\n",
    "           images - indexes of images to show. Randomly selected if None\n",
    "    RETURNS: Nothing.\n",
    "    '''\n",
    "    \n",
    "    num = 4\n",
    "    num_square = num ** 2\n",
    "    \n",
    "    if images is None:\n",
    "        images = np.random.randint(0, n_train, num_square)\n",
    "        \n",
    "    print('Showing training image indexes {}'.format(images))\n",
    "\n",
    "    fig, axes = plt.subplots(num,num, figsize=(6,6))\n",
    "    for idx, val in enumerate(images):\n",
    "        r, c = divmod(idx, num)\n",
    "        ax = axes[r][c]\n",
    "        ax.imshow(X[images[idx]], cmap=plt.cm.binary)\n",
    "        ax.annotate('Label: {}'.format(y[val]), xy=(1, 1))\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "show_label_images(X_train_input, y_train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import *\n",
    "\n",
    "def resize_image(image, img_size, back_size, back_color):\n",
    "    '''Resizes the image to `new_size`, pastes in center of backgrond\n",
    "    INPUT: image = Input image\n",
    "           img_size = tuple of images new dimensions\n",
    "           back_size = tuple of new image background dimensions\n",
    "           back_color = color to fill background with\n",
    "    RETURNS: New image\n",
    "    '''\n",
    "\n",
    "    new_img = Image.new('L', back_size, (back_color,))\n",
    "    img = Image.fromarray(image)\n",
    "    img.thumbnail(img_size, Image.ANTIALIAS) \n",
    "\n",
    "    h_diff = int((back_size[0] - img_size[1]) / 2)\n",
    "    w_diff = int((back_size[1] - img_size[1]) / 2)\n",
    "\n",
    "    new_img.paste(img, (h_diff, w_diff))\n",
    "    return np.asarray(new_img)\n",
    "\n",
    "def resize_image_array(X, img_size, back_size, back_color, desc=None):\n",
    "    '''Resizes images held in a numpy array'''\n",
    "    X_out = np.zeros_like(X)\n",
    "    n_images = X.shape[0]\n",
    "    for idx in tqdm(range(n_images), desc=desc): # \"Resizing {} images\".format(n_images)):\n",
    "        X_out[idx] = resize_image(X[idx], img_size, back_size, 0)\n",
    "        \n",
    "    return X_out\n",
    "\n",
    "X_train_resize = resize_image_array(X_train_input, (20,20), (28,28), 0, 'Resizing training images')\n",
    "X_test_resize = resize_image_array(X_test_input, (20,20), (28,28), 0, 'Resizing training images')\n",
    "\n",
    "# Reshape the images so they're a single row in the numpy array\n",
    "X_train_resize = X_train_resize_float.reshape((n_train, w * h))\n",
    "X_test_resize = X_test_resize_float.reshape((n_test, w * h))\n",
    "\n",
    "\n",
    "print('Z-normalizing X data')\n",
    "std = StandardScaler()\n",
    "X_train_resize_float = X_train_resize.astype(np.float32)\n",
    "X_test_resize_float = X_test_resize.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "std.fit(X_train_resize_float.astype(np.float32))\n",
    "X_train = std.transform(X_train_resize_float)\n",
    "X_test = std.transform(X_test_resize_float)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model a - 28x28-300-10: 4.7% Error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "print('Model summary:\\n')\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining model\\n')\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=NB_EPOCH,\n",
    "          batch_size=BATCH,\n",
    "          verbose=2)\n",
    "\n",
    "print('\\nGenerating predictions on test set\\n')\n",
    "scores = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "errors = [1.0 - score for score in scores]\n",
    "\n",
    "print('\\n\\nTest set training error {:.4f}, test error {:.4f}'.format(errors[0], errors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
