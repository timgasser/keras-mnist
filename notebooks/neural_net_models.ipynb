{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models\n",
    "\n",
    "This notebook picks up after the `simple_models` notebook. After trying a range of classification algorithms, we'll try out some of the neural network models in [1]. These include fully-cfully connected the results selected simple models from the [LeNet page](http://yann.lecun.com/exdb/mnist/) and performs setup as described in `[1]`, evaluating their performance on the MNIST classification task.  The evaluation uses 5-fold cross-validation with a stratified split strategy, and reports the error rate.\n",
    "\n",
    "`[1]` - [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf),  LeCun et al, Nov 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in dependencies, may not use all of these\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle files ...\n",
      "Loaded train images shape (60000, 28, 28), labels shape (60000, 1)\n",
      "Loaded test images shape (10000, 28, 28), labels shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set up the file directory and names\n",
    "DIR = '../input/'\n",
    "X_TRAIN = DIR + 'train-images-idx3-ubyte.pkl'\n",
    "Y_TRAIN = DIR + 'train-labels-idx1-ubyte.pkl'\n",
    "X_TEST = DIR + 't10k-images-idx3-ubyte.pkl'\n",
    "Y_TEST = DIR + 't10k-labels-idx1-ubyte.pkl'\n",
    "\n",
    "print('Loading pickle files ...')\n",
    "X_train = pickle.load( open( X_TRAIN, \"rb\" ) )\n",
    "y_train = pickle.load( open( Y_TRAIN, \"rb\" ) )\n",
    "X_test = pickle.load( open( X_TEST, \"rb\" ) )\n",
    "y_test = pickle.load( open( Y_TEST, \"rb\" ) )\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "w = X_train.shape[1]\n",
    "h = X_train.shape[2]\n",
    "\n",
    "# Reshape the images so they're a single row in the numpy array\n",
    "X_train_input = X_train.reshape((n_train, w * h))\n",
    "X_test_input = X_test.reshape((n_test, w * h))\n",
    "y_train_input = y_train.squeeze()\n",
    "y_test_input = y_test.squeeze()\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "print('Loaded train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Loaded test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Before trying a few different algorithms out, let's define a reusable set of functions to cross-validate and predict on the test set. Because scikit-learn has such a uniform interface, we can re-use these on pretty much any classification algorithm out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "\n",
    "def evaluate_model(model, scoring, X, y, n_folds, n_jobs, random_state):\n",
    "    ''' Evaluates the performance of a model using X and y with n_folds cross validation'''\n",
    "#     print('Running {}-fold cross validation on {} examples ..'.format(n_folds, count))\n",
    "    cv = StratifiedKFold(y, n_folds, shuffle=True, random_state=random_state)\n",
    "    accuracies = cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring=scoring, \n",
    "                                 n_jobs=n_jobs, verbose=0)\n",
    "    errors = [1 - acc for acc in accuracies]\n",
    "    return errors\n",
    "\n",
    "def train_model(model, X, y, random_state):\n",
    "    ''' Trains and returns model'''\n",
    "#     print('Training model with {} examples ..'.format(n_examples))\n",
    "    model_fit = model.fit(X, y)\n",
    "    return model_fit\n",
    "\n",
    "def train_and_predict(model, X_train, y_train, X_test, random_state):\n",
    "    ''' Trains and makes prediction'''\n",
    "    fit_model = train_model(model, X_train, y_train, random_state)\n",
    "    y_test_pred = fit_model.predict(X_test)\n",
    "    return y_test_pred\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, random_state):\n",
    "    ''' Trains the model, predicts on test set, evaluates performance'''\n",
    "    fit_model = train_model(model, X_train, y_train, random_state)\n",
    "    acc = fit_model.score(X_test, y_test)\n",
    "    error = 1.0 - acc\n",
    "    return error\n",
    "    \n",
    "def mean_std(values):\n",
    "    '''Calculates the mean and std dev of the input values\n",
    "    INPUT: List or numpy array of values\n",
    "    RETURNS (mean, std)\n",
    "    '''\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global setup\n",
    "\n",
    "There are a couple of different ways to speed up the training. We can spread the work over multiple cores (if the algorithm supports it), this is controlled in scikit-learn's `n_jobs` parameters. We can also reduce the training set, which will decrease the performance of algorithms. But if all algorithms are operating from the same reduced training dataset, we can still compare relative performance. Testing always has to be done on the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape (60000, 784), labels shape (60000,)\n",
      "Test images shape (10000, 784), labels shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_JOBS=-2 # Leave 1 core free for UI updates\n",
    "VERBOSE=1 # 3 is the most verbose level\n",
    "SEED = 1234 # Fix the seed for repeatability\n",
    "MAX_ITER = 100 # L-BFGS may show warnings that it doesn't converge\n",
    "N_FOLDS = 5 # How may folds to do k-folds cross validation for\n",
    "\n",
    "# Create a stratified, shuffled subset of the training data if needed\n",
    "N = n_train # How may training examples to use\n",
    "if N < n_train:\n",
    "    X_train, _, y_train, _ = train_test_split(X_train_input, y_train_input, \n",
    "                                          train_size=N, random_state=SEED)\n",
    "else:\n",
    "    X_train = X_train_input\n",
    "    y_train = y_train_input\n",
    "    \n",
    "X_test = X_test_input\n",
    "y_test = y_test_input\n",
    "\n",
    "print('Train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.1 - Logistic regression\n",
    "\n",
    "Logistic regression is a simple model to train that's a good baseline to work from. We're using the `multinomial` option for the model, which minimizes the error across the entire probability distribution. This most closely corresponds to the 'Pairwise linear' error rate of 7.6% in `[1]`.  Our model has a 5-fold CV error of 8.1%  on the training set, and test error of 7.49%. This is 0.11% different to the 7.6% error reported in [1] C.1, pretty close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.1s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.1s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.0s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.4s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression CV on 60000 examples. Mean error 0.080684, Std dev 0.002010\n",
      "Logistic regression error 0.074800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   27.9s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', \n",
    "                                  max_iter=MAX_ITER, \n",
    "                                  verbose=VERBOSE, random_state=SEED) \n",
    "\n",
    "errors = evaluate_model(model=logreg_model, scoring='accuracy', \n",
    "                        X=X_train, y=y_train, n_folds=N_FOLDS, \n",
    "                        n_jobs=N_JOBS, random_state=SEED)\n",
    "\n",
    "cv_error_mean, cv_error_std = mean_std(errors)\n",
    "\n",
    "test_error = train_and_evaluate(model=logreg_model, \n",
    "                                X_train=X_train, y_train=y_train, \n",
    "                                X_test=X_test, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "print('Logistic regression CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "print('Logistic regression error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.2 k-Nearest Neighbours\n",
    "\n",
    "Let's try kNN to classify the numbers.  This is a non-linear classification algorithm that accepts a single hyperparameter to control the number of neighbours, `k`.  [1] C.2. used  a `k` of 3, which gave an error rate of 5% on the test set.  When we ran kNN with k=3 and Euclidean distance metric the test error was 2.95%. This was slightly higher than the deslanted data result of 2.4% reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN CV on 60000 examples. Mean error 0.028500, Std dev 0.002017\n",
      "kNN test error 0.029500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='euclidean', p=1, n_jobs=N_JOBS)\n",
    "errors = evaluate_model(knn_model, 'accuracy', X_train, y_train,  \n",
    "                        n_folds=N_FOLDS, n_jobs=N_JOBS, random_state=SEED)\n",
    "cv_error_mean, cv_error_std = mean_std(errors)\n",
    "\n",
    "test_error = train_and_evaluate(model=knn_model, \n",
    "                                X_train=X_train, y_train=y_train, \n",
    "                                X_test=X_test, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "print('kNN CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "print('kNN test error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking performance of kNN with k = 1 ..\n",
      "Checking performance of kNN with k = 2 ..\n",
      "Checking performance of kNN with k = 3 ..\n",
      "Checking performance of kNN with k = 4 ..\n",
      "Checking performance of kNN with k = 5 ..\n",
      "kNN with k 1, test error 0.029500\n",
      "kNN with k 2, test error 0.029500\n",
      "kNN with k 3, test error 0.029500\n",
      "kNN with k 4, test error 0.029500\n",
      "kNN with k 5, test error 0.029500\n"
     ]
    }
   ],
   "source": [
    "# Let's try some different values for k .. \n",
    "scores = dict()\n",
    "\n",
    "# New idea - just train and predict on test set instead of doing CV\n",
    "for k in (1, 2, 3, 4, 5):\n",
    "    print('Checking performance of kNN with k = {} ..'.format(k))\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=3, n_jobs=N_JOBS)\n",
    "    test_error = train_and_evaluate(model=knn_model, \n",
    "                                    X_train=X_train, y_train=y_train, \n",
    "                                    X_test=X_test, y_test=y_test, \n",
    "                                    random_state=SEED)\n",
    "    scores[k] = test_error\n",
    "    \n",
    "result = {print('kNN with k {}, test error {:.6f}'.format(key, val)) for key, val in scores.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution is pretty evenly split between the classes. 1 is the most popular class with 11.24% of instances, and at the other end 5 is the least frequent class, with 9.04% of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## [1] C.3 PCA and Polynomial Classifier\n",
    "\n",
    "This section of the paper uses a series of pre-processing steps to help a logistic regression (similar to C.1). The pre-processing stages are:\n",
    "\n",
    "* Subtract the mean of each input component.\n",
    "* Do a PCA on the resulting data, keeping the top 40 features explaining the variance.\n",
    "* Add polynomial terms to the input features. The 40 input PCA features multiply up to 821\n",
    "* Train a multinomial logistic regression on the resulting 821 features.\n",
    "\n",
    "[1] C.3 reports a test set accuracy of 3.3%. Our model reports 3.2%, matching to within 0.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtracting mean from training data\n",
      "Projecting training data onto top 40 PCA basis vectors\n",
      "Generating 821 interaction terms between PCA data\n",
      "Training data shape (60000, 821)\n",
      "Subtracting mean of training data from test data\n",
      "Projecting test data onto top 40 PCA basis vectors\n",
      "Test data shape (10000, 821)\n",
      "Training logistic regression model on PCA interaction terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.7s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   37.0s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.2s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.7s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   42.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression CV on 60000 examples. Mean error 0.035067, Std dev 0.002260\n",
      "Logistic regression error 0.031900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "N_COMP=40\n",
    "\n",
    "### TRAIN DATA ###\n",
    "\n",
    "# Need to subtract the mean of the training data first\n",
    "print('Subtracting mean from training data')\n",
    "X_train_mean = np.mean(X_train, axis=0)\n",
    "X_train_pca = X_train - X_train_mean\n",
    "\n",
    "# now do PCA, keep first 40 components\n",
    "print('Projecting training data onto top {} PCA basis vectors'.format(N_COMP))\n",
    "pca = PCA(n_components=N_COMP)\n",
    "pca.fit(X_train_pca)\n",
    "X_train_pca = pca.transform(X_train_pca)\n",
    "\n",
    "# Create interaction terms between the 40 top PCA components\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "poly = poly.fit(X_train_pca)\n",
    "n_poly = poly.n_output_features_\n",
    "print('Generating {} interaction terms between PCA data'.format(n_poly))\n",
    "X_train_pca_poly = poly.transform(X_train_pca)\n",
    "print('Training data shape {}'.format(X_train_pca_poly.shape))\n",
    "\n",
    "### TEST DATA ###\n",
    "\n",
    "# Need to subtract the mean of the training data first\n",
    "print('Subtracting mean of training data from test data')\n",
    "# X_test_mean = np.mean(X_train, axis=0)\n",
    "X_test_pca = X_test - X_train_mean\n",
    "\n",
    "# Don't refit PCA to the test data ! Just transform\n",
    "print('Projecting test data onto top {} PCA basis vectors'.format(N_COMP))\n",
    "# pca = PCA(n_components=N_COMP)\n",
    "# pca.fit(X_test_pca)\n",
    "X_test_pca = pca.transform(X_test_pca)\n",
    "\n",
    "# Generate interaction polynomical terms\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "# poly = poly.fit(X_test_pca)\n",
    "X_test_pca_poly = poly.transform(X_test_pca)\n",
    "print('Test data shape {}'.format(X_test_pca_poly.shape))\n",
    "\n",
    "### MODEL ###\n",
    "print('Training logistic regression model on PCA interaction terms')\n",
    "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', \n",
    "                                  max_iter=MAX_ITER, \n",
    "                                  verbose=VERBOSE, random_state=SEED) \n",
    "\n",
    "errors = evaluate_model(model=logreg_model, scoring='accuracy', \n",
    "                        X=X_train_pca_poly, y=y_train, n_folds=N_FOLDS, \n",
    "                        n_jobs=N_JOBS, random_state=SEED)\n",
    "\n",
    "cv_error_mean, cv_error_std = mean_std(errors)\n",
    "\n",
    "test_error = train_and_evaluate(model=logreg_model, \n",
    "                                X_train=X_train_pca_poly, y_train=y_train, \n",
    "                                X_test=X_test_pca_poly, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "print('Logistic regression CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "print('Logistic regression error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.4  Radial Basis Function Network\n",
    "\n",
    "I'm going to skip this one for now.  RBFNs are fully connected neural nets that use RBFs as their activation function. I couldn't find anything similar in scikit learn, or Tensorflow. I might come back to this one later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1]  SVM Poly 4\n",
    "\n",
    "Although not mentioned explicitly in the sections of the paper,  Fig 9 on page 12 shows an `SVM poly 4` entry with an error of 1.1%. It would be a shame to miss out SVMs in a classification problem, so I'll check the performance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardising input features before SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n",
      "Evaluating SVM\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('Standardising input features before SVM')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print('Training SVM')\n",
    "clf = SVC(C=1.0, kernel='poly', degree=4, verbose=True)\n",
    "\n",
    "print('Evaluating SVM')\n",
    "errors = evaluate_model(clf, 'accuracy', X_train, y_train,  \n",
    "                        n_folds=N_FOLDS, n_jobs=N_JOBS, random_state=SEED)\n",
    "cv_error_mean, cv_error_std = mean_std(errors)\n",
    "print('SVM Poly-4 {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "\n",
    "clf.fit(X_train_std, y_train)\n",
    "accuracy = clf.score(X_test_std, y_test)\n",
    "error = 1.0 - accuracy\n",
    "print('SVM Poly-4  regression error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
