{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models\n",
    "\n",
    "This notebook picks selected simple models from the [LeNet page](http://yann.lecun.com/exdb/mnist/) and performs setup as described in `[1]`, evaluating their performance on the MNIST classification task.  The evaluation uses 5-fold cross-validation with a stratified split strategy, and reports the error rate.\n",
    "\n",
    "`[1]` - [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf),  LeCun et al, Nov 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in dependencies, may not use all of these\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle files ...\n",
      "Loaded train images shape (60000, 28, 28), labels shape (60000, 1)\n",
      "Loaded test images shape (10000, 28, 28), labels shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set up the file directory and names\n",
    "DIR = '../input/'\n",
    "X_TRAIN = DIR + 'train-images-idx3-ubyte.pkl'\n",
    "Y_TRAIN = DIR + 'train-labels-idx1-ubyte.pkl'\n",
    "X_TEST = DIR + 't10k-images-idx3-ubyte.pkl'\n",
    "Y_TEST = DIR + 't10k-labels-idx1-ubyte.pkl'\n",
    "\n",
    "print('Loading pickle files ...')\n",
    "X_train = pickle.load( open( X_TRAIN, \"rb\" ) )\n",
    "y_train = pickle.load( open( Y_TRAIN, \"rb\" ) )\n",
    "X_test = pickle.load( open( X_TEST, \"rb\" ) )\n",
    "y_test = pickle.load( open( Y_TEST, \"rb\" ) )\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "w = X_train.shape[1]\n",
    "h = X_train.shape[2]\n",
    "\n",
    "# Reshape the images so they're a single row in the numpy array\n",
    "X_train_input = X_train.reshape((n_train, w * h))\n",
    "X_test_input = X_test.reshape((n_test, w * h))\n",
    "y_train_input = y_train.squeeze()\n",
    "y_test_input = y_test.squeeze()\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "print('Loaded train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Loaded test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Before trying a few different algorithms out, let's define a reusable set of functions to cross-validate and predict on the test set. Because scikit-learn has such a uniform interface, we can re-use these on pretty much any classification algorithm out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "\n",
    "def evaluate_model(model, scoring, X, y, n_folds, n_jobs, random_state):\n",
    "    ''' Evaluates the performance of a model using X and y with n_folds cross validation'''\n",
    "#     print('Running {}-fold cross validation on {} examples ..'.format(n_folds, count))\n",
    "    cv = StratifiedKFold(y, n_folds, shuffle=True, random_state=random_state)\n",
    "    accuracies = cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring=scoring, \n",
    "                                 n_jobs=n_jobs, verbose=0)\n",
    "    errors = [1 - acc for acc in accuracies]\n",
    "    return errors\n",
    "\n",
    "def train_model(model, X, y, random_state):\n",
    "    ''' Trains and returns model'''\n",
    "#     print('Training model with {} examples ..'.format(n_examples))\n",
    "    model_fit = model.fit(X, y)\n",
    "    return model_fit\n",
    "\n",
    "def train_and_predict(model, X_train, y_train, X_test, random_state):\n",
    "    ''' Trains and makes prediction'''\n",
    "    fit_model = train_model(model, X_train, y_train, random_state)\n",
    "    y_test_pred = fit_model.predict(X_test)\n",
    "    return y_test_pred\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, random_state):\n",
    "    ''' Trains the model, predicts on test set, evaluates performance'''\n",
    "    fit_model = train_model(model, X_train, y_train, random_state)\n",
    "    acc = fit_model.score(X_test, y_test)\n",
    "    error = 1.0 - acc\n",
    "    return error\n",
    "    \n",
    "def mean_std(values):\n",
    "    '''Calculates the mean and std dev of the input values\n",
    "    INPUT: List or numpy array of values\n",
    "    RETURNS (mean, std)\n",
    "    '''\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global setup\n",
    "\n",
    "There are a couple of different ways to speed up the training. We can spread the work over multiple cores (if the algorithm supports it), this is controlled in scikit-learn's `n_jobs` parameters. We can also reduce the training set, which will decrease the performance of algorithms. But if all algorithms are operating from the same reduced training dataset, we can still compare relative performance. Testing always has to be done on the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape (60000, 784), labels shape (60000,)\n",
      "Test images shape (10000, 784), labels shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_JOBS=-2 # Leave 1 core free for UI updates\n",
    "VERBOSE=1 # 3 is the most verbose level\n",
    "SEED = 1234 # Fix the seed for repeatability\n",
    "MAX_ITER = 100 # L-BFGS may show warnings that it doesn't converge\n",
    "N_FOLDS = 5 # How may folds to do k-folds cross validation for\n",
    "\n",
    "# Create a stratified, shuffled subset of the training data if needed\n",
    "N = n_train # How may training examples to use\n",
    "if N < n_train:\n",
    "    X_train, _, y_train, _ = train_test_split(X_train_input, y_train_input, \n",
    "                                          train_size=N, random_state=SEED)\n",
    "else:\n",
    "    X_train = X_train_input\n",
    "    y_train = y_train_input\n",
    "    \n",
    "X_test = X_test_input\n",
    "y_test = y_test_input\n",
    "\n",
    "print('Train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.1 - Logistic regression\n",
    "\n",
    "Logistic regression is a simple model to train that's a good baseline to work from. We're using the `multinomial` option for the model, which minimizes the error across the entire probability distribution. This most closely corresponds to the 'Pairwise linear' error rate of 7.6% in `[1]`.  Our model has a 5-fold CV error of 8.1%  on the training set, and test error of 7.49%. This is 0.11% different to the 7.6% error reported in [1] C.1, pretty close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.9s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.9s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.8s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.4s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression CV on 60000 examples. Mean error 0.080684, Std dev 0.002010\n",
      "Logistic regression error 0.074800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', \n",
    "                                  max_iter=MAX_ITER, \n",
    "                                  verbose=VERBOSE, random_state=SEED) \n",
    "\n",
    "errors = evaluate_model(model=logreg_model, scoring='accuracy', \n",
    "                        X=X_train, y=y_train, n_folds=N_FOLDS, \n",
    "                        n_jobs=N_JOBS, random_state=SEED)\n",
    "\n",
    "cv_error_mean, cv_error_std = mean_std(errors)\n",
    "\n",
    "test_error = train_and_evaluate(model=logreg_model, \n",
    "                                X_train=X_train, y_train=y_train, \n",
    "                                X_test=X_test, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "print('Logistic regression CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "print('Logistic regression error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.2 k-Nearest Neighbours\n",
    "\n",
    "Let's try kNN to classify the numbers.  This is a non-linear classification algorithm that accepts a single hyperparameter to control the number of neighbours, `k`.  [1] C.2. used  a `k` of 3, which gave an error rate of 5% on the test set.  When we ran kNN with k=3 and Euclidean distance metric the test error was 2.95%. This was slightly higher than the deslanted data result of 2.4% reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN test error 0.029500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='euclidean', p=1, n_jobs=N_JOBS)\n",
    "errors = evaluate_model(knn_model, 'accuracy', X_train, y_train, NUM_EXAMPLES, \n",
    "                        n_folds=5, n_jobs=-2, random_state=SEED)\n",
    "cv_error_mean, cv_error_std = mean_std(errors)\n",
    "\n",
    "test_error = train_and_evaluate(model=knn_model, \n",
    "                                X_train=X_train, y_train=y_train, \n",
    "                                X_test=X_test, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "print('kNN CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "print('kNN test error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking performance of kNN with k = 1 ..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-55efe22b5eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                     \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                     \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                     random_state=SEED)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1b5a6bcb3610>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, X_train, y_train, X_test, y_test, random_state)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m''' Trains the model, predicts on test set, evaluates performance'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    380\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m             )\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tim/anaconda/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's try some different values for k .. \n",
    "scores = dict()\n",
    "\n",
    "# New idea - just train and predict on test set instead of doing CV\n",
    "for k in (1, 2, 3, 4, 5):\n",
    "    print('Checking performance of kNN with k = {} ..'.format(k))\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=3, n_jobs=N_JOBS)\n",
    "    test_error = train_and_evaluate(model=knn_model, \n",
    "                                    X_train=X_train, y_train=y_train, \n",
    "                                    X_test=X_test, y_test=y_test, \n",
    "                                    random_state=SEED)\n",
    "    scores[k] = test_error\n",
    "    \n",
    "result = {print('kNN with k {}, test error {:.6f}'.format(key, val)) for key, val in scores.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution is pretty evenly split between the classes. 1 is the most popular class with 11.24% of instances, and at the other end 5 is the least frequent class, with 9.04% of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## [1] C.3 PCA and Polynomial Classifier\n",
    "\n",
    "This section of the paper uses a series of pre-processing steps to help a logistic regression (similar to C.1). The pre-processing stages are:\n",
    "\n",
    "* Subtract the mean of each input component.\n",
    "* Do a PCA on the resulting data, keeping the top 40 features explaining the variance.\n",
    "* Add polynomial terms to the input features. The 40 input PCA features multiply up to 821\n",
    "* Train a multinomial logistic regression on the resulting 821 features.\n",
    "\n",
    "[1] C.3 reports a test set accuracy of 3.3%. Our model reports 3.2%, matching to within 0.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtracting mean from training data\n",
      "Projecting training data onto top 40 PCA basis vectors\n",
      "Generating 821 interaction terms between PCA data\n",
      "Training data shape (60000, 821)\n",
      "Subtracting mean of training data from test data\n",
      "Projecting test data onto top 40 PCA basis vectors\n",
      "Test data shape (10000, 821)\n",
      "Training logistic regression model on PCA interaction terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.9s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.9s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   30.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   30.8s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.1s finished\n",
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression CV on 60000 examples. Mean error 0.035101, Std dev 0.002214\n",
      "Logistic regression error 0.032000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "N_COMP=40\n",
    "\n",
    "### TRAIN DATA ###\n",
    "\n",
    "# Need to subtract the mean of the training data first\n",
    "print('Subtracting mean from training data')\n",
    "X_train_mean = np.mean(X_train, axis=0)\n",
    "X_train_pca = X_train - X_train_mean\n",
    "\n",
    "# now do PCA, keep first 40 components\n",
    "print('Projecting training data onto top {} PCA basis vectors'.format(N_COMP))\n",
    "pca = PCA(n_components=N_COMP)\n",
    "pca.fit(X_train_pca)\n",
    "X_train_pca = pca.transform(X_train_pca)\n",
    "\n",
    "# Create interaction terms between the 40 top PCA components\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "poly = poly.fit(X_train_pca)\n",
    "n_poly = poly.n_output_features_\n",
    "print('Generating {} interaction terms between PCA data'.format(n_poly))\n",
    "X_train_pca_poly = poly.transform(X_train_pca)\n",
    "print('Training data shape {}'.format(X_train_pca_poly.shape))\n",
    "\n",
    "### TEST DATA ###\n",
    "\n",
    "# Need to subtract the mean of the training data first\n",
    "print('Subtracting mean of training data from test data')\n",
    "# X_test_mean = np.mean(X_train, axis=0)\n",
    "X_test_pca = X_test - X_train_mean\n",
    "\n",
    "# Don't refit PCA to the test data ! Just transform\n",
    "print('Projecting test data onto top {} PCA basis vectors'.format(N_COMP))\n",
    "# pca = PCA(n_components=N_COMP)\n",
    "# pca.fit(X_test_pca)\n",
    "X_test_pca = pca.transform(X_test_pca)\n",
    "\n",
    "# Generate interaction polynomical terms\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "# poly = poly.fit(X_test_pca)\n",
    "X_test_pca_poly = poly.transform(X_test_pca)\n",
    "print('Test data shape {}'.format(X_test_pca_poly.shape))\n",
    "\n",
    "### MODEL ###\n",
    "print('Training logistic regression model on PCA interaction terms')\n",
    "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', \n",
    "                                  max_iter=MAX_ITER, \n",
    "                                  verbose=VERBOSE, random_state=SEED) \n",
    "\n",
    "errors = evaluate_model(model=logreg_model, scoring='accuracy', \n",
    "                        X=X_train_pca_poly, y=y_train, n_folds=N_FOLDS, \n",
    "                        n_jobs=N_JOBS, random_state=SEED)\n",
    "\n",
    "cv_error_mean, cv_error_std = mean_std(errors)\n",
    "\n",
    "test_error = train_and_evaluate(model=logreg_model, \n",
    "                                X_train=X_train_pca_poly, y_train=y_train, \n",
    "                                X_test=X_test_pca_poly, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "print('Logistic regression CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "print('Logistic regression error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.4  Radial Basis Function Network\n",
    "\n",
    "I'm going to skip this one for now.  RBFNs are fully connected neural nets that use RBFs as their activation function. I couldn't find anything similar in scikit learn, or Tensorflow. I might come back to this one later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1]  SVM Poly 4\n",
    "\n",
    "Although not mentioned explicitly in the sections of the paper,  Fig 9 on page 12 shows an `SVM poly 4` entry with an error of 1.1%. It would be a shame to miss out SVMs in a classification problem, so I'll check the performance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardising input features before SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('Standardising input features before SVM')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print('Training SVM')\n",
    "clf = SVC(C=1.0, kernel='poly', degree=4, verbose=True)\n",
    "clf.fit(X_train_std, y_train)\n",
    "\n",
    "print('Evaluating SVM')\n",
    "clf.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
