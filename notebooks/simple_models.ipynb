{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models\n",
    "\n",
    "This notebook picks selected simple models from the [LeNet page](http://yann.lecun.com/exdb/mnist/) and performs setup as described in `[1]`, evaluating their performance on the MNIST classification task.  The evaluation uses 5-fold cross-validation with a stratified split strategy, and reports the error rate.\n",
    "\n",
    "`[1]` - [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf),  LeCun et al, Nov 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "# plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Helvetica'\n",
    "plt.rcParams['font.monospace'] = 'Consolas'\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle files ...\n",
      "Loaded train images shape (60000, 28, 28), labels shape (60000, 1)\n",
      "Loaded test images shape (10000, 28, 28), labels shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set up the file directory and names\n",
    "DIR = '../input/'\n",
    "X_TRAIN = DIR + 'train-images-idx3-ubyte.pkl'\n",
    "Y_TRAIN = DIR + 'train-labels-idx1-ubyte.pkl'\n",
    "X_TEST = DIR + 't10k-images-idx3-ubyte.pkl'\n",
    "Y_TEST = DIR + 't10k-labels-idx1-ubyte.pkl'\n",
    "\n",
    "print('Loading pickle files ...')\n",
    "X_train = pickle.load( open( X_TRAIN, \"rb\" ) )\n",
    "y_train = pickle.load( open( Y_TRAIN, \"rb\" ) )\n",
    "X_test = pickle.load( open( X_TEST, \"rb\" ) )\n",
    "y_test = pickle.load( open( Y_TEST, \"rb\" ) )\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "w = X_train.shape[1]\n",
    "h = X_train.shape[2]\n",
    "\n",
    "# Reshape the images so they're a single row in the numpy array\n",
    "X_train_input = X_train.reshape((n_train, w * h))\n",
    "X_test_input = X_test.reshape((n_test, w * h))\n",
    "y_train_input = y_train.squeeze()\n",
    "y_test_input = y_test.squeeze()\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "print('Loaded train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Loaded test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Before trying a few different algorithms out, let's define a reusable set of functions to cross-validate and predict on the test set. Because scikit-learn has such a uniform interface, we can re-use these on pretty much any classification algorithm out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def evaluate_model(model, scoring, X, y, n_folds, n_jobs, random_state):\n",
    "    ''' Evaluates the performance of a model using X and y with n_folds cross validation'''\n",
    "#     print('Running {}-fold cross validation on {} examples ..'.format(n_folds, count))\n",
    "    cv = StratifiedKFold(n_folds, shuffle=True, random_state=random_state)\n",
    "    accuracies = cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring=scoring, \n",
    "                                 n_jobs=n_jobs, verbose=0)\n",
    "    errors = [1 - acc for acc in accuracies]\n",
    "    return errors\n",
    "\n",
    "def train_model(model, X, y, random_state):\n",
    "    ''' Trains and returns model'''\n",
    "#     print('Training model with {} examples ..'.format(n_examples))\n",
    "    model_fit = model.fit(X, y)\n",
    "    return model_fit\n",
    "\n",
    "def train_and_predict(model, X_train, y_train, X_test, random_state):\n",
    "    ''' Trains and makes prediction'''\n",
    "    fit_model = train_model(model, X_train, y_train, random_state)\n",
    "    y_test_pred = fit_model.predict(X_test)\n",
    "    return y_test_pred\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, random_state):\n",
    "    ''' Trains the model, predicts on test set, evaluates performance'''\n",
    "    fit_model = train_model(model, X_train, y_train, random_state)\n",
    "    acc = fit_model.score(X_test, y_test)\n",
    "    error = 1.0 - acc\n",
    "    return error\n",
    "    \n",
    "def mean_std(values):\n",
    "    '''Calculates the mean and std dev of the input values\n",
    "    INPUT: List or numpy array of values\n",
    "    RETURNS (mean, std)\n",
    "    '''\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global setup\n",
    "\n",
    "There are a couple of different ways to speed up the training. We can spread the work over multiple cores (if the algorithm supports it), this is controlled in scikit-learn's `n_jobs` parameters. We can also reduce the training set, which will decrease the performance of algorithms. But if all algorithms are operating from the same reduced training dataset, we can still compare relative performance. Testing always has to be done on the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape (60000, 784), labels shape (60000,)\n",
      "Test images shape (10000, 784), labels shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_JOBS=-2 # Leave 1 core free for UI updates\n",
    "VERBOSE=1 # 3 is the most verbose level\n",
    "SEED = 1234 # Fix the seed for repeatability\n",
    "MAX_ITER = 100 # L-BFGS may show warnings that it doesn't converge\n",
    "N_FOLDS = 5 # How may folds to do k-folds cross validation for\n",
    "RUN_CV = True # Run Cross-validation on training set?\n",
    "\n",
    "scores = dict() # Store the scores in here\n",
    "\n",
    "# Create a stratified, shuffled subset of the training data if needed\n",
    "N = n_train # How may training examples to use\n",
    "if N < n_train:\n",
    "    X_train, _, y_train, _ = train_test_split(X_train_input, y_train_input, \n",
    "                                          train_size=N, random_state=SEED)\n",
    "else:\n",
    "    X_train = X_train_input\n",
    "    y_train = y_train_input\n",
    "    \n",
    "X_test = X_test_input\n",
    "y_test = y_test_input\n",
    "\n",
    "print('Train images shape {}, labels shape {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test images shape {}, labels shape {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.1 - Logistic regression\n",
    "\n",
    "Logistic regression is a simple model to train that's a good baseline to work from. We're using the `multinomial` option for the model, which minimizes the error across the entire probability distribution. This most closely corresponds to the 'Pairwise linear' error rate of 7.6% in `[1]`.  Our model has a 5-fold CV error of 8.1%  on the training set, and test error of 7.49%. This is 0.11% different to the 7.6% error reported in [1] C.1, pretty close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   21.8s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   22.4s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   22.6s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.5s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression CV on 60000 examples. Mean error 0.080684, Std dev 0.002010\n",
      "Logistic regression error 0.074800\n",
      "CPU times: user 30.2 s, sys: 336 ms, total: 30.6 s\n",
      "Wall time: 54.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.5s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', \n",
    "                                  max_iter=MAX_ITER, \n",
    "                                  verbose=VERBOSE, random_state=SEED) \n",
    "\n",
    "if RUN_CV:\n",
    "    errors = evaluate_model(model=logreg_model, scoring='accuracy', # evaluate_model converts to errors\n",
    "                            X=X_train, y=y_train, n_folds=N_FOLDS, \n",
    "                            n_jobs=N_JOBS, random_state=SEED)\n",
    "\n",
    "    cv_error_mean, cv_error_std = mean_std(errors)\n",
    "    print('Logistic regression CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "\n",
    "test_error = train_and_evaluate(model=logreg_model, \n",
    "                                X_train=X_train, y_train=y_train, \n",
    "                                X_test=X_test, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "scores['log_reg'] = test_error\n",
    "print('Logistic regression error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.2 k-Nearest Neighbours\n",
    "\n",
    "Let's try kNN to classify the numbers.  This is a non-linear classification algorithm that accepts a single hyperparameter to control the number of neighbours, `k`.  [1] C.2. used  a `k` of 3, which gave an error rate of 5% on the test set.  When we ran kNN with k=3 and Euclidean distance metric the test error was 2.95%. This was slightly higher than the deslanted data result of 2.4% reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN CV on 60000 examples. Mean error 0.028500, Std dev 0.002017\n",
      "kNN test error 0.029500\n",
      "CPU times: user 55min 45s, sys: 1.7 s, total: 55min 46s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='euclidean', p=1, n_jobs=N_JOBS)\n",
    "\n",
    "if RUN_CV: \n",
    "    errors = evaluate_model(knn_model, 'accuracy', X_train, y_train,  \n",
    "                            n_folds=N_FOLDS, n_jobs=N_JOBS, random_state=SEED)\n",
    "    cv_error_mean, cv_error_std = mean_std(errors)\n",
    "    print('kNN CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "\n",
    "test_error = train_and_evaluate(model=knn_model, \n",
    "                                X_train=X_train, y_train=y_train, \n",
    "                                X_test=X_test, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "scores['knn'] = test_error\n",
    "print('kNN test error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.3 PCA and Polynomial Classifier\n",
    "\n",
    "This section of the paper uses a series of pre-processing steps to help a logistic regression (similar to C.1). The pre-processing stages are:\n",
    "\n",
    "* Subtract the mean of each input component.\n",
    "* Do a PCA on the resulting data, keeping the top 40 features explaining the variance.\n",
    "* Add polynomial terms to the input features. The 40 input PCA features multiply up to 821\n",
    "* Train a multinomial logistic regression on the resulting 821 features.\n",
    "\n",
    "[1] C.3 reports a test set accuracy of 3.3%. Our model reports 3.2%, matching to within 0.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtracting mean from training data\n",
      "Projecting training data onto top 40 PCA basis vectors\n",
      "Generating 821 interaction terms between PCA data\n",
      "Training data shape (60000, 821)\n",
      "Subtracting mean of training data from test data\n",
      "Projecting test data onto top 40 PCA basis vectors\n",
      "Test data shape (10000, 821)\n",
      "Training logistic regression model on PCA interaction terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   27.6s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.0s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.8s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.1s finished\n",
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   43.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression CV on 60000 examples. Mean error 0.035017, Std dev 0.002435\n",
      "Logistic regression error 0.031900\n",
      "CPU times: user 1min 26s, sys: 47.2 s, total: 2min 13s\n",
      "Wall time: 2min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "N_COMP=40\n",
    "\n",
    "### TRAIN DATA ###\n",
    "\n",
    "# Need to subtract the mean of the training data first\n",
    "print('Subtracting mean from training data')\n",
    "X_train_mean = np.mean(X_train, axis=0)\n",
    "X_train_pca = X_train - X_train_mean\n",
    "\n",
    "# now do PCA, keep first 40 components\n",
    "print('Projecting training data onto top {} PCA basis vectors'.format(N_COMP))\n",
    "pca = PCA(n_components=N_COMP)\n",
    "pca.fit(X_train_pca)\n",
    "X_train_pca = pca.transform(X_train_pca)\n",
    "\n",
    "# Create interaction terms between the 40 top PCA components\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "poly = poly.fit(X_train_pca)\n",
    "n_poly = poly.n_output_features_\n",
    "print('Generating {} interaction terms between PCA data'.format(n_poly))\n",
    "X_train_pca_poly = poly.transform(X_train_pca)\n",
    "print('Training data shape {}'.format(X_train_pca_poly.shape))\n",
    "\n",
    "### TEST DATA ###\n",
    "\n",
    "# Need to subtract the mean of the training data first\n",
    "print('Subtracting mean of training data from test data')\n",
    "# X_test_mean = np.mean(X_train, axis=0)\n",
    "X_test_pca = X_test - X_train_mean\n",
    "\n",
    "# Don't refit PCA to the test data ! Just transform\n",
    "print('Projecting test data onto top {} PCA basis vectors'.format(N_COMP))\n",
    "# pca = PCA(n_components=N_COMP)\n",
    "# pca.fit(X_test_pca)\n",
    "X_test_pca = pca.transform(X_test_pca)\n",
    "\n",
    "# Generate interaction polynomical terms\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "# poly = poly.fit(X_test_pca)\n",
    "X_test_pca_poly = poly.transform(X_test_pca)\n",
    "print('Test data shape {}'.format(X_test_pca_poly.shape))\n",
    "\n",
    "### MODEL ###\n",
    "print('Training logistic regression model on PCA interaction terms')\n",
    "\n",
    "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', \n",
    "                                  max_iter=MAX_ITER, \n",
    "                                  verbose=VERBOSE, random_state=SEED) \n",
    "\n",
    "if RUN_CV:\n",
    "    errors = evaluate_model(model=logreg_model, scoring='accuracy', \n",
    "                            X=X_train_pca_poly, y=y_train, n_folds=N_FOLDS, \n",
    "                            n_jobs=N_JOBS, random_state=SEED)\n",
    "\n",
    "    cv_error_mean, cv_error_std = mean_std(errors)\n",
    "    print('Logistic regression CV on {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "  \n",
    "\n",
    "test_error = train_and_evaluate(model=logreg_model, \n",
    "                                X_train=X_train_pca_poly, y_train=y_train, \n",
    "                                X_test=X_test_pca_poly, y_test=y_test, \n",
    "                                random_state=SEED)\n",
    "\n",
    "scores['pca_int_logreg'] = test_error\n",
    "print('Logistic regression error {:.6f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] C.4  Radial Basis Function Network\n",
    "\n",
    "I'm going to skip this one for now.  RBFNs are fully connected neural nets that use RBFs as their activation function. I couldn't find anything similar in scikit learn, or Tensorflow. I might come back to this one later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1]  SVM Poly 4\n",
    "\n",
    "Although not mentioned explicitly in the sections of the paper,  Fig 9 on page 12 shows an `SVM poly 4` entry with an error of 1.1%. It would be a shame to miss out SVMs in a classification problem, so I'll check the performance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardising input features before SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n",
      "Evaluating SVM\n",
      "SVM Poly-4 60000 examples. Mean error 0.028333, Std dev 0.000950\n",
      "[LibSVM]SVM Poly-4  regression error 0.132400\n",
      "CPU times: user 50min 25s, sys: 12.4 s, total: 50min 37s\n",
      "Wall time: 55min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('Standardising input features before SVM')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print('Training SVM')\n",
    "clf = SVC(C=1.0, kernel='poly', degree=4, verbose=True)\n",
    "\n",
    "# Cross validating SVM on training data will take ~60mins\n",
    "print('Evaluating SVM')\n",
    "if RUN_CV:\n",
    "    errors = evaluate_model(clf, 'accuracy', X_train, y_train,  \n",
    "                            n_folds=N_FOLDS, n_jobs=N_JOBS, random_state=SEED)\n",
    "    cv_error_mean, cv_error_std = mean_std(errors)\n",
    "    print('SVM Poly-4 {} examples. Mean error {:.6f}, Std dev {:.6f}'.format(N, cv_error_mean, cv_error_std))\n",
    "\n",
    "clf.fit(X_train_std, y_train)\n",
    "accuracy = clf.score(X_test_std, y_test)\n",
    "error = 1.0 - accuracy\n",
    "\n",
    "scores['svm_poly4'] = error\n",
    "print('SVM Poly-4  regression error {:.6f}'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of scores for simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca_int_logreg</th>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_reg</th>\n",
       "      <td>7.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_poly4</th>\n",
       "      <td>13.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                error\n",
       "knn              2.95\n",
       "pca_int_logreg   3.19\n",
       "log_reg          7.48\n",
       "svm_poly4       13.24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_scores_df = pd.DataFrame.from_dict(scores, orient='index')\n",
    "simple_scores_df.columns = ['error']\n",
    "simple_scores_df['error'] *= 100\n",
    "simple_scores_df = simple_scores_df.sort_values('error', ascending=True)\n",
    "simple_scores_df.to_pickle('simple_scores.pkl')\n",
    "simple_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAGRCAYAAACAOAGXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff4a640ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "simple_scores_df.plot.barh(width=0.4, ax=ax, legend=None)\n",
    "ax.set(title=\"Simple model test-set accuracy\", ylabel=\"Algorithm\", xlabel=\"%age error\");\n",
    "plt.savefig('simple_scores.png', bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
